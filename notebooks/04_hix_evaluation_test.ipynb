{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HIX-Based LLM Evaluation Test\n",
        "\n",
        "This notebook implements the HIX-style scoring methodology to evaluate LLMs on Easy Language conversion.\n",
        "\n",
        "## Objective\n",
        "Test 2 LLMs on their ability to simplify complex German texts following Easy Language rules, scored using HIX-like metrics.\n",
        "\n",
        "## Models to Test\n",
        "- `qwen/qwen3-32b` - Strong multilingual model\n",
        "- `llama-3.3-70b-versatile` - Proven reliable model\n",
        "\n",
        "## Evaluation Criteria\n",
        "- **HIX Components**: Sentence length, word length, clause structure\n",
        "- **Guardrails**: Max 20 words/sentence, avoid passive voice, avoid negations\n",
        "- **Score Range**: 0-20 (higher = easier to read)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Setup & Dependencies\n",
        "%pip install pandas groq python-dotenv --quiet\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from groq import Groq\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    print(\"ERROR: GROQ_API_KEY not found\")\n",
        "else:\n",
        "    client = Groq(api_key=api_key)\n",
        "    print(\"Connected to Groq API\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Configuration\n",
        "\n",
        "# Models to test\n",
        "MODELS = [\n",
        "    \"qwen/qwen3-32b\",\n",
        "    \"llama-3.3-70b-versatile\"\n",
        "]\n",
        "\n",
        "# Complex German test texts (from samples)\n",
        "TEST_TEXTS = {\n",
        "    \"legal\": \"\"\"Das Bundesministerium der Justiz und für Verbraucherschutz und das Bundesamt für Justiz stellen für interessierte Bürgerinnen und Bürger nahezu das gesamte aktuelle Bundesrecht kostenlos im Internet bereit. Die Gesetze und Rechtsverordnungen können in ihrer jeweils geltenden Fassung abgerufen werden. Sie werden durch die Dokumentationsstelle im Bundesamt für Justiz fortlaufend konsolidiert.\"\"\",\n",
        "    \n",
        "    \"academic\": \"\"\"Die Arbeit analysiert das politische Framing in den Bundestagsdebatten rund um die Einführung und Abschaffung der sogenannten Praxisgebühr. Da Framing begrifflich und methodisch uneinheitlich verwendet wird, wurde ein politischer Framing-Ansatz hergeleitet. Insbesondere der konflikthaften Dimension politischen Framings kam bislang nur wenig Aufmerksamkeit zu.\"\"\"\n",
        "}\n",
        "\n",
        "# Reference benchmarks (from Target corpus - Easy Language)\n",
        "# These would normally be computed from a real corpus\n",
        "BENCHMARKS = {\n",
        "    \"target\": {  # P50 from Easy Language corpus\n",
        "        \"avg_sentence_length\": 12.0,\n",
        "        \"avg_word_length\": 5.5,\n",
        "        \"pct_long_sentences\": 0.05,  # 5% sentences > 20 words\n",
        "        \"pct_long_words\": 0.15       # 15% words > 6 chars\n",
        "    },\n",
        "    \"negative\": {  # P50 from Hard corpus\n",
        "        \"avg_sentence_length\": 28.0,\n",
        "        \"avg_word_length\": 7.5,\n",
        "        \"pct_long_sentences\": 0.60,  # 60% sentences > 20 words\n",
        "        \"pct_long_words\": 0.45       # 45% words > 6 chars\n",
        "    }\n",
        "}\n",
        "\n",
        "# Guardrail thresholds\n",
        "GUARDRAILS = {\n",
        "    \"max_pct_long_sentences\": 0.10,  # Max 10% sentences > 20 words\n",
        "    \"max_passive_rate\": 0.15,        # Max 15% passive constructions\n",
        "    \"max_negation_rate\": 0.10        # Max 10% negation words\n",
        "}\n",
        "\n",
        "print(f\"Models: {MODELS}\")\n",
        "print(f\"Test texts: {list(TEST_TEXTS.keys())}\")\n",
        "print(f\"Benchmarks defined for scoring\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. HIX Metrics Functions\n",
        "\n",
        "def split_sentences(text: str) -> list:\n",
        "    \"\"\"Split German text into sentences.\"\"\"\n",
        "    # Handle common German abbreviations\n",
        "    text = re.sub(r'\\bz\\.B\\.', 'zB', text)\n",
        "    text = re.sub(r'\\bd\\.h\\.', 'dh', text)\n",
        "    text = re.sub(r'\\busw\\.', 'usw', text)\n",
        "    text = re.sub(r'\\bggfs\\.', 'ggfs', text)\n",
        "    \n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "\n",
        "def get_words(text: str) -> list:\n",
        "    \"\"\"Extract words from text.\"\"\"\n",
        "    return re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "\n",
        "def compute_metrics(text: str) -> dict:\n",
        "    \"\"\"Compute HIX-style metrics for a text.\"\"\"\n",
        "    sentences = split_sentences(text)\n",
        "    words = get_words(text)\n",
        "    \n",
        "    if not sentences or not words:\n",
        "        return {\"error\": \"No content\"}\n",
        "    \n",
        "    # Basic counts\n",
        "    n_sentences = len(sentences)\n",
        "    n_words = len(words)\n",
        "    \n",
        "    # Sentence lengths\n",
        "    sent_lengths = [len(get_words(s)) for s in sentences]\n",
        "    avg_sent_len = sum(sent_lengths) / n_sentences\n",
        "    long_sents = sum(1 for l in sent_lengths if l > 20)\n",
        "    pct_long_sents = long_sents / n_sentences\n",
        "    \n",
        "    # Word lengths\n",
        "    word_lengths = [len(w) for w in words]\n",
        "    avg_word_len = sum(word_lengths) / n_words\n",
        "    long_words = sum(1 for l in word_lengths if l > 6)\n",
        "    pct_long_words = long_words / n_words\n",
        "    \n",
        "    # Passive voice detection (German: \"wird/werden/wurde/wurden + Partizip II\")\n",
        "    passive_patterns = r'\\b(wird|werden|wurde|wurden|worden)\\b'\n",
        "    passive_matches = len(re.findall(passive_patterns, text, re.IGNORECASE))\n",
        "    passive_rate = passive_matches / n_sentences\n",
        "    \n",
        "    # Negation detection\n",
        "    negation_patterns = r'\\b(nicht|kein|keine|keiner|keinem|keinen|nie|niemals|nichts)\\b'\n",
        "    negation_matches = len(re.findall(negation_patterns, text, re.IGNORECASE))\n",
        "    negation_rate = negation_matches / n_words\n",
        "    \n",
        "    return {\n",
        "        \"n_sentences\": n_sentences,\n",
        "        \"n_words\": n_words,\n",
        "        \"avg_sentence_length\": round(avg_sent_len, 1),\n",
        "        \"avg_word_length\": round(avg_word_len, 1),\n",
        "        \"pct_long_sentences\": round(pct_long_sents, 3),\n",
        "        \"pct_long_words\": round(pct_long_words, 3),\n",
        "        \"passive_rate\": round(passive_rate, 3),\n",
        "        \"negation_rate\": round(negation_rate, 3)\n",
        "    }\n",
        "\n",
        "\n",
        "def clamp(value: float, min_val: float, max_val: float) -> float:\n",
        "    \"\"\"Clamp value to range.\"\"\"\n",
        "    return max(min_val, min(max_val, value))\n",
        "\n",
        "\n",
        "def compute_hix_score(metrics: dict) -> dict:\n",
        "    \"\"\"Compute HIX-like score (0-20) from metrics.\"\"\"\n",
        "    target = BENCHMARKS[\"target\"]\n",
        "    neg = BENCHMARKS[\"negative\"]\n",
        "    \n",
        "    scores = {}\n",
        "    \n",
        "    # Score each metric (lower is better for all these)\n",
        "    for metric in [\"avg_sentence_length\", \"avg_word_length\", \"pct_long_sentences\", \"pct_long_words\"]:\n",
        "        x = metrics.get(metric, neg[metric])\n",
        "        t = target[metric]\n",
        "        n = neg[metric]\n",
        "        \n",
        "        if n != t:\n",
        "            raw_score = 10 * (n - x) / (n - t)\n",
        "            scores[f\"score_{metric}\"] = round(clamp(raw_score, 0, 10), 2)\n",
        "        else:\n",
        "            scores[f\"score_{metric}\"] = 5.0\n",
        "    \n",
        "    # Combine scores (simplified HIX: just average all)\n",
        "    all_scores = [v for k, v in scores.items() if k.startswith(\"score_\")]\n",
        "    hix_like = sum(all_scores) / len(all_scores) * 2  # Scale to 0-20\n",
        "    \n",
        "    scores[\"hix_like\"] = round(hix_like, 2)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def check_guardrails(metrics: dict) -> dict:\n",
        "    \"\"\"Check if metrics pass guardrail thresholds.\"\"\"\n",
        "    checks = {\n",
        "        \"pass_long_sentences\": metrics.get(\"pct_long_sentences\", 1.0) <= GUARDRAILS[\"max_pct_long_sentences\"],\n",
        "        \"pass_passive\": metrics.get(\"passive_rate\", 1.0) <= GUARDRAILS[\"max_passive_rate\"],\n",
        "        \"pass_negation\": metrics.get(\"negation_rate\", 1.0) <= GUARDRAILS[\"max_negation_rate\"]\n",
        "    }\n",
        "    checks[\"all_passed\"] = all(checks.values())\n",
        "    return checks\n",
        "\n",
        "\n",
        "print(\"HIX metrics functions defined:\")\n",
        "print(\"  - compute_metrics(text)\")\n",
        "print(\"  - compute_hix_score(metrics)\")\n",
        "print(\"  - check_guardrails(metrics)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. LLM Simplification Function\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Du bist Experte für Einfache Sprache (Plain German / Leichte Sprache).\n",
        "Vereinfache den folgenden Text nach diesen Regeln:\n",
        "\n",
        "SATZEBENE:\n",
        "- Maximal 15-20 Wörter pro Satz\n",
        "- Höchstens ein Komma pro Satz\n",
        "- Keine Schachtelsätze oder Einschübe\n",
        "- Aktive Verben verwenden, Passiv vermeiden\n",
        "- Klare, eindeutige Sätze\n",
        "\n",
        "WORTEBENE:\n",
        "- Keine Fremdwörter\n",
        "- Schwierige Wörter erklären\n",
        "- Lange Wörter mit Bindestrich trennen\n",
        "- Keine Redewendungen oder Metaphern\n",
        "- Abkürzungen ausschreiben\n",
        "- Verneinungen vermeiden\n",
        "\n",
        "WICHTIG:\n",
        "- Gib NUR den vereinfachten Text aus\n",
        "- Behalte alle wichtigen Informationen\n",
        "- Keine Erklärungen oder Kommentare\"\"\"\n",
        "\n",
        "\n",
        "def simplify_text(text: str, model: str) -> str:\n",
        "    \"\"\"Simplify text using specified LLM.\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ],\n",
        "            temperature=0.2,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "        return completion.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"ERROR: {e}\"\n",
        "\n",
        "\n",
        "print(\"LLM simplification function ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Baseline: Analyze Original Texts\n",
        "\n",
        "print(\"BASELINE: Original Text Metrics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for text_type, text in TEST_TEXTS.items():\n",
        "    print(f\"\\n[{text_type.upper()}]\")\n",
        "    print(f\"Text: {text[:100]}...\")\n",
        "    \n",
        "    metrics = compute_metrics(text)\n",
        "    scores = compute_hix_score(metrics)\n",
        "    guardrails = check_guardrails(metrics)\n",
        "    \n",
        "    print(f\"\\nMetrics:\")\n",
        "    print(f\"  Sentences: {metrics['n_sentences']}, Words: {metrics['n_words']}\")\n",
        "    print(f\"  Avg sentence length: {metrics['avg_sentence_length']} words\")\n",
        "    print(f\"  Avg word length: {metrics['avg_word_length']} chars\")\n",
        "    print(f\"  % long sentences (>20 words): {metrics['pct_long_sentences']*100:.1f}%\")\n",
        "    print(f\"  Passive rate: {metrics['passive_rate']*100:.1f}%\")\n",
        "    \n",
        "    print(f\"\\nHIX-like Score: {scores['hix_like']}/20\")\n",
        "    print(f\"Guardrails passed: {guardrails['all_passed']}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Run LLM Evaluation\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"LLM SIMPLIFICATION TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for text_type, original_text in TEST_TEXTS.items():\n",
        "    print(f\"\\n[{text_type.upper()}] Testing models...\")\n",
        "    \n",
        "    # Get original metrics for comparison\n",
        "    orig_metrics = compute_metrics(original_text)\n",
        "    orig_scores = compute_hix_score(orig_metrics)\n",
        "    \n",
        "    for model in MODELS:\n",
        "        model_short = model.split(\"/\")[-1]\n",
        "        print(f\"  {model_short}...\", end=\" \", flush=True)\n",
        "        \n",
        "        # Simplify\n",
        "        simplified = simplify_text(original_text, model)\n",
        "        \n",
        "        if simplified.startswith(\"ERROR\"):\n",
        "            print(f\"Failed: {simplified}\")\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        simp_metrics = compute_metrics(simplified)\n",
        "        simp_scores = compute_hix_score(simp_metrics)\n",
        "        guardrails = check_guardrails(simp_metrics)\n",
        "        \n",
        "        # Store result\n",
        "        result = {\n",
        "            \"text_type\": text_type,\n",
        "            \"model\": model,\n",
        "            \"original\": original_text,\n",
        "            \"simplified\": simplified,\n",
        "            \"orig_hix\": orig_scores[\"hix_like\"],\n",
        "            \"simp_hix\": simp_scores[\"hix_like\"],\n",
        "            \"hix_improvement\": simp_scores[\"hix_like\"] - orig_scores[\"hix_like\"],\n",
        "            \"orig_avg_sent_len\": orig_metrics[\"avg_sentence_length\"],\n",
        "            \"simp_avg_sent_len\": simp_metrics[\"avg_sentence_length\"],\n",
        "            \"guardrails_passed\": guardrails[\"all_passed\"],\n",
        "            **{f\"guard_{k}\": v for k, v in guardrails.items()}\n",
        "        }\n",
        "        results.append(result)\n",
        "        \n",
        "        print(f\"HIX: {orig_scores['hix_like']} -> {simp_scores['hix_like']} (+{result['hix_improvement']:.1f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Results Summary\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Summary by model\n",
        "summary = df.groupby(\"model\").agg({\n",
        "    \"simp_hix\": \"mean\",\n",
        "    \"hix_improvement\": \"mean\",\n",
        "    \"simp_avg_sent_len\": \"mean\",\n",
        "    \"guardrails_passed\": \"mean\"\n",
        "}).round(2)\n",
        "\n",
        "summary.columns = [\"Avg HIX Score\", \"Avg Improvement\", \"Avg Sent Len\", \"Guardrails Pass %\"]\n",
        "summary[\"Guardrails Pass %\"] = (summary[\"Guardrails Pass %\"] * 100).astype(int).astype(str) + \"%\"\n",
        "\n",
        "display(summary)\n",
        "\n",
        "# Best model\n",
        "best_model = df.groupby(\"model\")[\"simp_hix\"].mean().idxmax()\n",
        "best_score = df.groupby(\"model\")[\"simp_hix\"].mean().max()\n",
        "print(f\"\\nBest Model: {best_model}\")\n",
        "print(f\"Average HIX Score: {best_score:.2f}/20\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Detailed Output Comparison\n",
        "\n",
        "print(\"DETAILED OUTPUT COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    model_short = row[\"model\"].split(\"/\")[-1]\n",
        "    print(f\"\\n[{row['text_type'].upper()}] Model: {model_short}\")\n",
        "    print(f\"HIX: {row['orig_hix']} -> {row['simp_hix']} (improvement: +{row['hix_improvement']:.1f})\")\n",
        "    print(f\"Sentence length: {row['orig_avg_sent_len']} -> {row['simp_avg_sent_len']} words\")\n",
        "    print(f\"Guardrails: {'PASSED' if row['guardrails_passed'] else 'FAILED'}\")\n",
        "    \n",
        "    print(f\"\\nOriginal ({len(row['original'])} chars):\")\n",
        "    print(f\"  {row['original'][:150]}...\")\n",
        "    \n",
        "    print(f\"\\nSimplified ({len(row['simplified'])} chars):\")\n",
        "    print(f\"  {row['simplified'][:300]}...\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. Save Results\n",
        "\n",
        "output_path = Path(\"../data/processed/hix_evaluation_results.csv\")\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"Results saved to: {output_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HIX EVALUATION TEST COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"\n",
        "Summary:\n",
        "- Tested {len(MODELS)} models on {len(TEST_TEXTS)} text types\n",
        "- Used HIX-style scoring (0-20 scale)\n",
        "- Applied guardrails: max 10% long sentences, max 15% passive, max 10% negations\n",
        "\n",
        "Next Steps:\n",
        "1. Expand test corpus with more texts\n",
        "2. Compute real benchmarks from DEplain corpus\n",
        "3. Add more models to comparison\n",
        "4. Fine-tune best performing model\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
