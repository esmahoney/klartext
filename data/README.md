# Data Directory

This directory contains datasets for training and evaluating KlarText simplification models.

## Structure

```
data/
├── raw/           # Original datasets (not committed - too large)
├── processed/     # Cleaned & preprocessed data (generated by notebooks)
├── samples/       # Small samples for testing (committed)
└── README.md
```

## Datasets to Explore

### German Easy Language
- **DEplain** — German plain language corpus
- **Klexikon** — Simple German Wikipedia for children
- **GEOLino** — Children's science articles
- **APA/Capito** — Austrian news in easy language

### English Plain Language
- **Newsela** — News articles at multiple reading levels
- **WikiLarge/WikiSmall** — Wikipedia simplification pairs
- **SimpleText** — Scientific text simplification

### Multilingual
- **mC4** — Multilingual Common Crawl (for pre-training)
- **OPUS** — Parallel corpora for translation

## Downloading Data

Large datasets should be downloaded separately. Example:

```bash
# Create a download script or use HuggingFace datasets
python scripts/download_data.py --dataset deplain
```

## Data Format

After preprocessing, data should be in JSONL format:

```json
{"source": "Complex German text...", "target": "Einfacher Text...", "level": "easy", "lang": "de"}
{"source": "Another complex text...", "target": "Simpler version...", "level": "very_easy", "lang": "de"}
```

## Privacy & Compliance

- Do NOT commit any data containing PII
- Check dataset licenses before commercial use
- Document data provenance in notebooks

