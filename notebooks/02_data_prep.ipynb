{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Data Preparation\n",
        "\n",
        "This notebook handles data cleaning, preprocessing, and train/val/test splitting.\n",
        "\n",
        "## Goals\n",
        "- Clean and filter raw datasets\n",
        "- Normalize text (whitespace, encoding)\n",
        "- Create train/validation/test splits\n",
        "- Export in format suitable for fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "\n",
        "print(f\"Raw data: {DATA_RAW}\")\n",
        "print(f\"Processed data: {DATA_PROCESSED}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Text Cleaning Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean and normalize text.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    \n",
        "    # Remove control characters (keep newlines for structure)\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "def filter_example(row: pd.Series, min_source_len: int = 20, max_source_len: int = 2000) -> bool:\n",
        "    \"\"\"Return True if example should be kept.\"\"\"\n",
        "    source = row.get(\"source\", \"\")\n",
        "    target = row.get(\"target\", \"\")\n",
        "    \n",
        "    # Must have both source and target\n",
        "    if not source or not target:\n",
        "        return False\n",
        "    \n",
        "    # Length constraints\n",
        "    if len(source) < min_source_len or len(source) > max_source_len:\n",
        "        return False\n",
        "    \n",
        "    # Target should be shorter or similar length (not way longer)\n",
        "    if len(target) > len(source) * 1.5:\n",
        "        return False\n",
        "    \n",
        "    # Source and target should be different\n",
        "    if source.strip() == target.strip():\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "\n",
        "# Test the functions\n",
        "test_text = \"  This   has   extra   whitespace.  \"\n",
        "print(f\"Original: '{test_text}'\")\n",
        "print(f\"Cleaned: '{clean_text(test_text)}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Clean Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load your actual dataset here\n",
        "# Example with sample data:\n",
        "\n",
        "sample_data = [\n",
        "    {\"source\": \"Complex legal text here...\", \"target\": \"Simple version.\", \"lang\": \"de\", \"level\": \"easy\"},\n",
        "    {\"source\": \"Another complex text...\", \"target\": \"Easier to read.\", \"lang\": \"de\", \"level\": \"easy\"},\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "\n",
        "# Clean texts\n",
        "df[\"source\"] = df[\"source\"].apply(clean_text)\n",
        "df[\"target\"] = df[\"target\"].apply(clean_text)\n",
        "\n",
        "# Filter\n",
        "df[\"keep\"] = df.apply(filter_example, axis=1)\n",
        "print(f\"Before filter: {len(df)}\")\n",
        "df = df[df[\"keep\"]].drop(columns=[\"keep\"])\n",
        "print(f\"After filter: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train/Val/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_splits(df: pd.DataFrame, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
        "    \"\"\"Split data into train/val/test sets.\"\"\"\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 0.001\n",
        "    \n",
        "    # First split: train vs (val + test)\n",
        "    train_df, temp_df = train_test_split(df, train_size=train_ratio, random_state=seed)\n",
        "    \n",
        "    # Second split: val vs test\n",
        "    relative_val = val_ratio / (val_ratio + test_ratio)\n",
        "    val_df, test_df = train_test_split(temp_df, train_size=relative_val, random_state=seed)\n",
        "    \n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# Create splits (only if we have enough data)\n",
        "if len(df) >= 3:\n",
        "    train_df, val_df, test_df = create_splits(df)\n",
        "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "else:\n",
        "    print(\"Not enough data for splitting. Add more examples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Export to JSONL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_jsonl(df: pd.DataFrame, path: Path):\n",
        "    \"\"\"Export dataframe to JSONL format.\"\"\"\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for _, row in df.iterrows():\n",
        "            json.dump(row.to_dict(), f, ensure_ascii=False)\n",
        "            f.write('\\n')\n",
        "    print(f\"Exported {len(df)} examples to {path}\")\n",
        "\n",
        "# Export (uncomment when you have real data)\n",
        "# export_jsonl(train_df, DATA_PROCESSED / \"train.jsonl\")\n",
        "# export_jsonl(val_df, DATA_PROCESSED / \"val.jsonl\")\n",
        "# export_jsonl(test_df, DATA_PROCESSED / \"test.jsonl\")\n",
        "\n",
        "print(\"Ready to export. Uncomment the lines above when you have real data.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
