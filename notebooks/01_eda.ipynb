{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Exploratory Data Analysis (EDA)\n",
        "\n",
        "This notebook explores datasets for training KlarText simplification models.\n",
        "\n",
        "## Goals\n",
        "- Understand available datasets (German & English)\n",
        "- Analyze text length distributions\n",
        "- Check language quality and edge cases\n",
        "- Identify potential issues before training\n",
        "\n",
        "## Datasets to Explore\n",
        "- DEplain (German plain language)\n",
        "- Klexikon (Simple German Wikipedia)\n",
        "- Newsela (English multi-level news)\n",
        "- WikiLarge (English Wikipedia simplification)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "# Optional: HuggingFace datasets\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# Set paths\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "DATA_SAMPLES = PROJECT_ROOT / \"data\" / \"samples\"\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data directories exist: raw={DATA_RAW.exists()}, processed={DATA_PROCESSED.exists()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Datasets\n",
        "\n",
        "Load datasets from HuggingFace Hub or local files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Load a dataset from HuggingFace\n",
        "# Uncomment and modify as needed\n",
        "\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# # German: DEplain corpus\n",
        "# deplain = load_dataset(\"DEplain/DEplain-web\", split=\"train\")\n",
        "# print(f\"DEplain: {len(deplain)} examples\")\n",
        "# print(deplain[0])\n",
        "\n",
        "# # English: WikiLarge\n",
        "# wikilarge = load_dataset(\"wiki_lingua\", \"english\", split=\"train\")\n",
        "\n",
        "# For now, create sample data to demonstrate the analysis\n",
        "sample_data = [\n",
        "    {\n",
        "        \"source\": \"Der Antragsteller muss die erforderlichen Unterlagen innerhalb der gesetzlich vorgeschriebenen Frist einreichen, andernfalls wird der Antrag als unvollständig abgelehnt.\",\n",
        "        \"target\": \"Sie müssen die Papiere rechtzeitig abgeben. Sonst wird Ihr Antrag abgelehnt.\",\n",
        "        \"level\": \"easy\",\n",
        "        \"lang\": \"de\"\n",
        "    },\n",
        "    {\n",
        "        \"source\": \"The implementation of the proposed regulatory framework necessitates comprehensive stakeholder engagement and iterative refinement of operational procedures.\",\n",
        "        \"target\": \"We need to talk to everyone involved. Then we can improve how things work step by step.\",\n",
        "        \"level\": \"easy\", \n",
        "        \"lang\": \"en\"\n",
        "    },\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "print(f\"Sample data: {len(df)} examples\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Length Analysis\n",
        "\n",
        "Analyze the length of source and target texts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_text_lengths(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Add text length columns to dataframe.\"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"source_chars\"] = df[\"source\"].str.len()\n",
        "    df[\"target_chars\"] = df[\"target\"].str.len()\n",
        "    df[\"source_words\"] = df[\"source\"].str.split().str.len()\n",
        "    df[\"target_words\"] = df[\"target\"].str.split().str.len()\n",
        "    df[\"compression_ratio\"] = df[\"target_chars\"] / df[\"source_chars\"]\n",
        "    return df\n",
        "\n",
        "df = analyze_text_lengths(df)\n",
        "print(\"Length statistics:\")\n",
        "df[[\"source_chars\", \"target_chars\", \"source_words\", \"target_words\", \"compression_ratio\"]].describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Length distributions\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "# Source vs Target length\n",
        "axes[0].scatter(df[\"source_words\"], df[\"target_words\"], alpha=0.6)\n",
        "axes[0].plot([0, df[\"source_words\"].max()], [0, df[\"source_words\"].max()], 'r--', label='Equal length')\n",
        "axes[0].set_xlabel(\"Source words\")\n",
        "axes[0].set_ylabel(\"Target words\")\n",
        "axes[0].set_title(\"Source vs Target Length\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Compression ratio by language\n",
        "if len(df) > 1:\n",
        "    df.groupby(\"lang\")[\"compression_ratio\"].mean().plot(kind=\"bar\", ax=axes[1])\n",
        "    axes[1].set_title(\"Avg Compression Ratio by Language\")\n",
        "    axes[1].set_ylabel(\"Ratio (target/source)\")\n",
        "\n",
        "# Word count distribution\n",
        "axes[2].hist(df[\"source_words\"], bins=20, alpha=0.5, label=\"Source\")\n",
        "axes[2].hist(df[\"target_words\"], bins=20, alpha=0.5, label=\"Target\")\n",
        "axes[2].set_xlabel(\"Word count\")\n",
        "axes[2].set_title(\"Word Count Distribution\")\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Vocabulary Analysis\n",
        "\n",
        "Analyze word frequency and vocabulary differences between source and target.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vocabulary_stats(texts: pd.Series, name: str = \"\"):\n",
        "    \"\"\"Get vocabulary statistics for a series of texts.\"\"\"\n",
        "    all_words = \" \".join(texts).lower().split()\n",
        "    word_freq = Counter(all_words)\n",
        "    \n",
        "    print(f\"\\n{name} Vocabulary Stats:\")\n",
        "    print(f\"  Total words: {len(all_words):,}\")\n",
        "    print(f\"  Unique words: {len(word_freq):,}\")\n",
        "    print(f\"  Top 10 words: {word_freq.most_common(10)}\")\n",
        "    \n",
        "    return word_freq\n",
        "\n",
        "source_vocab = get_vocabulary_stats(df[\"source\"], \"Source\")\n",
        "target_vocab = get_vocabulary_stats(df[\"target\"], \"Target\")\n",
        "\n",
        "# Words only in source (complex words that got simplified)\n",
        "source_only = set(source_vocab.keys()) - set(target_vocab.keys())\n",
        "print(f\"\\nWords in source but not target (simplified away): {list(source_only)[:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Quality Checks\n",
        "\n",
        "Check for potential data quality issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quality_checks(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"Run quality checks on the dataset.\"\"\"\n",
        "    issues = {}\n",
        "    \n",
        "    # Empty texts\n",
        "    issues[\"empty_source\"] = df[\"source\"].str.strip().eq(\"\").sum()\n",
        "    issues[\"empty_target\"] = df[\"target\"].str.strip().eq(\"\").sum()\n",
        "    \n",
        "    # Very short texts (< 10 chars)\n",
        "    issues[\"short_source\"] = (df[\"source\"].str.len() < 10).sum()\n",
        "    issues[\"short_target\"] = (df[\"target\"].str.len() < 10).sum()\n",
        "    \n",
        "    # Target longer than source (unusual for simplification)\n",
        "    issues[\"target_longer\"] = (df[\"target_chars\"] > df[\"source_chars\"]).sum()\n",
        "    \n",
        "    # Duplicates\n",
        "    issues[\"duplicate_source\"] = df[\"source\"].duplicated().sum()\n",
        "    issues[\"duplicate_pairs\"] = df.duplicated(subset=[\"source\", \"target\"]).sum()\n",
        "    \n",
        "    # Source == Target (no simplification)\n",
        "    issues[\"identical_pairs\"] = (df[\"source\"] == df[\"target\"]).sum()\n",
        "    \n",
        "    return issues\n",
        "\n",
        "issues = quality_checks(df)\n",
        "print(\"Quality Check Results:\")\n",
        "for check, count in issues.items():\n",
        "    status = \"✅\" if count == 0 else \"⚠️\"\n",
        "    print(f\"  {status} {check}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Next Steps\n",
        "\n",
        "Based on this EDA:\n",
        "- [ ] Download full datasets (see `data/README.md`)\n",
        "- [ ] Filter out low-quality examples\n",
        "- [ ] Proceed to `02_data_prep.ipynb` for preprocessing\n",
        "- [ ] Consider which simplification levels to support\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
