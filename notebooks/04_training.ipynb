{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Training / Fine-Tuning\n",
        "\n",
        "This notebook handles fine-tuning a model for text simplification.\n",
        "\n",
        "## Overview\n",
        "1. Load preprocessed data from `data/processed/`\n",
        "2. Configure training hyperparameters\n",
        "3. Fine-tune with HuggingFace Trainer\n",
        "4. Evaluate on test set\n",
        "5. Export model for inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Uncomment for training\n",
        "# import torch\n",
        "# from transformers import (\n",
        "#     AutoTokenizer,\n",
        "#     AutoModelForSeq2SeqLM,\n",
        "#     Seq2SeqTrainer,\n",
        "#     Seq2SeqTrainingArguments,\n",
        "#     DataCollatorForSeq2Seq,\n",
        "# )\n",
        "# from datasets import Dataset\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
        "\n",
        "print(f\"Data: {DATA_PROCESSED}\")\n",
        "print(f\"Models will be saved to: {MODELS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "CONFIG = {\n",
        "    # Model\n",
        "    \"base_model\": \"google/mt5-small\",  # or mt5-base, flan-t5-base\n",
        "    \"max_source_length\": 512,\n",
        "    \"max_target_length\": 256,\n",
        "    \n",
        "    # Training\n",
        "    \"batch_size\": 8,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"num_epochs\": 3,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \n",
        "    # Outputs\n",
        "    \"output_dir\": str(MODELS_DIR / \"klartext-mt5-small\"),\n",
        "    \"logging_steps\": 100,\n",
        "    \"save_steps\": 500,\n",
        "    \"eval_steps\": 500,\n",
        "}\n",
        "\n",
        "print(\"Training config:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_jsonl(path: Path) -> list:\n",
        "    \"\"\"Load JSONL file.\"\"\"\n",
        "    data = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Load data (uncomment when you have processed data)\n",
        "# train_data = load_jsonl(DATA_PROCESSED / \"train.jsonl\")\n",
        "# val_data = load_jsonl(DATA_PROCESSED / \"val.jsonl\")\n",
        "# test_data = load_jsonl(DATA_PROCESSED / \"test.jsonl\")\n",
        "\n",
        "# print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "\n",
        "print(\"Data loading ready (uncomment when processed data exists)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment for actual training\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"base_model\"])\n",
        "\n",
        "# def preprocess_function(examples):\n",
        "#     \"\"\"Tokenize source and target texts.\"\"\"\n",
        "#     # Add task prefix\n",
        "#     inputs = [f\"simplify: {text}\" for text in examples[\"source\"]]\n",
        "#     targets = examples[\"target\"]\n",
        "#     \n",
        "#     model_inputs = tokenizer(\n",
        "#         inputs, \n",
        "#         max_length=CONFIG[\"max_source_length\"], \n",
        "#         truncation=True,\n",
        "#         padding=\"max_length\",\n",
        "#     )\n",
        "#     \n",
        "#     labels = tokenizer(\n",
        "#         targets,\n",
        "#         max_length=CONFIG[\"max_target_length\"],\n",
        "#         truncation=True,\n",
        "#         padding=\"max_length\",\n",
        "#     )\n",
        "#     \n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs\n",
        "\n",
        "# # Create datasets\n",
        "# train_dataset = Dataset.from_list(train_data).map(preprocess_function, batched=True)\n",
        "# val_dataset = Dataset.from_list(val_data).map(preprocess_function, batched=True)\n",
        "\n",
        "print(\"Preprocessing functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment for actual training\n",
        "\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(CONFIG[\"base_model\"])\n",
        "\n",
        "# training_args = Seq2SeqTrainingArguments(\n",
        "#     output_dir=CONFIG[\"output_dir\"],\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     eval_steps=CONFIG[\"eval_steps\"],\n",
        "#     learning_rate=CONFIG[\"learning_rate\"],\n",
        "#     per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
        "#     per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
        "#     weight_decay=CONFIG[\"weight_decay\"],\n",
        "#     save_total_limit=3,\n",
        "#     num_train_epochs=CONFIG[\"num_epochs\"],\n",
        "#     predict_with_generate=True,\n",
        "#     logging_steps=CONFIG[\"logging_steps\"],\n",
        "#     save_steps=CONFIG[\"save_steps\"],\n",
        "#     warmup_steps=CONFIG[\"warmup_steps\"],\n",
        "#     fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "# )\n",
        "\n",
        "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# trainer = Seq2SeqTrainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator=data_collator,\n",
        "# )\n",
        "\n",
        "# # Train!\n",
        "# trainer.train()\n",
        "\n",
        "# # Save final model\n",
        "# trainer.save_model(CONFIG[\"output_dir\"])\n",
        "# tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
        "\n",
        "print(\"Training code ready (uncomment to run)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set (uncomment after training)\n",
        "\n",
        "# def generate_simplification(text: str, model, tokenizer, max_length=256):\n",
        "#     inputs = tokenizer(f\"simplify: {text}\", return_tensors=\"pt\", truncation=True)\n",
        "#     outputs = model.generate(**inputs, max_length=max_length, num_beams=4)\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# # Load trained model\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(CONFIG[\"output_dir\"])\n",
        "# tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"output_dir\"])\n",
        "\n",
        "# # Test on a few examples\n",
        "# for example in test_data[:5]:\n",
        "#     output = generate_simplification(example[\"source\"], model, tokenizer)\n",
        "#     print(f\"Source: {example['source'][:100]}...\")\n",
        "#     print(f\"Target: {example['target'][:100]}...\")\n",
        "#     print(f\"Output: {output[:100]}...\")\n",
        "#     print(\"-\" * 50)\n",
        "\n",
        "print(\"Evaluation code ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Export for Production\n",
        "\n",
        "After training, export the model for use in the KlarText API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export options:\n",
        "\n",
        "# 1. HuggingFace Hub (recommended for sharing)\n",
        "# model.push_to_hub(\"klartext/klartext-mt5-small\")\n",
        "# tokenizer.push_to_hub(\"klartext/klartext-mt5-small\")\n",
        "\n",
        "# 2. ONNX export (for faster inference)\n",
        "# from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
        "# ort_model = ORTModelForSeq2SeqLM.from_pretrained(CONFIG[\"output_dir\"], export=True)\n",
        "# ort_model.save_pretrained(CONFIG[\"output_dir\"] + \"-onnx\")\n",
        "\n",
        "# 3. Quantization (for smaller model size)\n",
        "# from transformers import AutoModelForSeq2SeqLM\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(CONFIG[\"output_dir\"])\n",
        "# model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "print(\"Export options documented. Uncomment preferred method after training.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
